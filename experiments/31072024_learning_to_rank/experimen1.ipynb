{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "# add root folder to path\n",
    "folder = \"../../\"\n",
    "sys.path.append(folder)\n",
    "from src.utils import load_data\n",
    "from src.utils import plot_metrics_grid\n",
    "from src.utils import load_baseline_rec_result\n",
    "from src.metrics import evaluate_recommender_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  movie_id  rating  timestamp\n",
       "0              1      1193       5  978300760\n",
       "1              1       661       3  978302109\n",
       "2              1       914       3  978301968\n",
       "3              1      3408       4  978300275\n",
       "4              1      2355       5  978824291\n",
       "...          ...       ...     ...        ...\n",
       "1000204     6040      1091       1  956716541\n",
       "1000205     6040      1094       5  956704887\n",
       "1000206     6040       562       5  956704746\n",
       "1000207     6040      1096       4  956715648\n",
       "1000208     6040      1097       4  956715569\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users,ratings,movies = load_data('../../data/ml-1m')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# import itertools\n",
    "\n",
    "# # Assuming users, ratings, and movies are already loaded as DataFrames\n",
    "# # Merge the datasets\n",
    "\n",
    "# # Convert categorical columns to numeric\n",
    "# label_encoders = {}\n",
    "# for column in ['gender', 'occupation', 'genres']:\n",
    "#     le = LabelEncoder()\n",
    "#     data[column] = le.fit_transform(data[column])\n",
    "#     label_encoders[column] = le\n",
    "\n",
    "# # Extract year from title\n",
    "# data['year'] = data['title'].str.extract(r'\\((\\d{4})\\)').astype(float)\n",
    "# data = data.drop(columns=['title', 'timestamp', 'zip'])\n",
    "\n",
    "# # Fill missing year values with the median year\n",
    "# data['year'].fillna(data['year'].median(), inplace=True)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Generate pairs for pairwise ranking\n",
    "# def generate_pairs(data, k=100):\n",
    "#     pairs = []\n",
    "#     grouped = data.groupby('user_id')\n",
    "#     for _, group in grouped:\n",
    "#         group = group.sample(frac=1).reset_index(drop=True)  # Shuffle the group\n",
    "#         n = len(group)\n",
    "#         if n < 2:\n",
    "#             continue\n",
    "#         pair_indices = list(itertools.combinations(range(n), 2))\n",
    "#         pair_indices = pair_indices[:k]  # Limit to k pairs per user\n",
    "#         for i, j in pair_indices:\n",
    "#             pairs.append((group.iloc[i], group.iloc[j]))\n",
    "#     return pairs\n",
    "\n",
    "# train_pairs = generate_pairs(train_data, k=100)\n",
    "# test_pairs = generate_pairs(test_data, k=100)\n",
    "\n",
    "# # Define dataset class for pairs\n",
    "# class PairwiseMovieDataset(Dataset):\n",
    "#     def __init__(self, pairs):\n",
    "#         self.pairs = pairs\n",
    "#         self.scaler = StandardScaler()\n",
    "#         all_data = pd.concat([pd.DataFrame([pair[0], pair[1]]) for pair in pairs])\n",
    "#         self.scaler.fit(all_data[['age', 'year']])\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.pairs)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         pos_item, neg_item = self.pairs[idx]\n",
    "#         pos_user_features = torch.tensor([pos_item['gender'], pos_item['age'], pos_item['occupation']], dtype=torch.float)\n",
    "#         pos_movie_features = torch.tensor([pos_item['genres'], pos_item['year']], dtype=torch.float)\n",
    "#         neg_user_features = torch.tensor([neg_item['gender'], neg_item['age'], neg_item['occupation']], dtype=torch.float)\n",
    "#         neg_movie_features = torch.tensor([neg_item['genres'], neg_item['year']], dtype=torch.float)\n",
    "        \n",
    "#         pos_user_features[1] = torch.tensor(self.scaler.transform(pos_user_features[1].reshape(1, -1)), dtype=torch.float)\n",
    "#         neg_user_features[1] = torch.tensor(self.scaler.transform(neg_user_features[1].reshape(1, -1)), dtype=torch.float)\n",
    "        \n",
    "#         return pos_user_features, pos_movie_features, neg_user_features, neg_movie_features\n",
    "\n",
    "# # Create DataLoader\n",
    "# train_dataset = PairwiseMovieDataset(train_pairs)\n",
    "# test_dataset = PairwiseMovieDataset(test_pairs)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39842/432861178.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['year'].fillna(data['year'].median(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>year</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children's</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416292</th>\n",
       "      <td>2507</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683230</th>\n",
       "      <td>4087</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688533</th>\n",
       "      <td>4118</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472584</th>\n",
       "      <td>2907</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  rating  gender  age  occupation    year  Action  Adventure  \\\n",
       "416292     2507       2    True   25           4  1955.0   False      False   \n",
       "683230     4087       4    True    1           4  1999.0   False      False   \n",
       "2434         19       3    True    1          10  1993.0    True      False   \n",
       "688533     4118       4    True   25           3  1983.0   False      False   \n",
       "472584     2907       4   False   35           5  1996.0   False      False   \n",
       "\n",
       "        Animation  Children's  ...  Fantasy  Film-Noir  Horror  Musical  \\\n",
       "416292      False       False  ...    False      False   False    False   \n",
       "683230      False       False  ...    False      False   False    False   \n",
       "2434        False       False  ...    False      False   False    False   \n",
       "688533      False       False  ...    False      False   False    False   \n",
       "472584      False       False  ...    False      False   False    False   \n",
       "\n",
       "        Mystery  Romance  Sci-Fi  Thriller    War  Western  \n",
       "416292    False    False   False     False   True    False  \n",
       "683230    False    False   False      True  False    False  \n",
       "2434      False    False   False      True  False    False  \n",
       "688533    False    False   False     False  False    False  \n",
       "472584    False    False   False     False  False    False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Merge the datasets\n",
    "data = ratings.merge(users, on='user_id').merge(movies, on='movie_id')\n",
    "\n",
    "# Extract year from title\n",
    "data['year'] = data['title'].str.extract(r'\\((\\d{4})\\)').astype(float)\n",
    "\n",
    "# Split genres into separate columns\n",
    "data['genres'] = data['genres'].str.split('|')\n",
    "\n",
    "# Create a DataFrame for each unique genre and merge them into the main DataFrame\n",
    "genres_expanded = data['genres'].explode().unique()\n",
    "genre_columns = pd.get_dummies(data['genres'].explode()).groupby(level=0).max()\n",
    "\n",
    "# Join the new genre columns to the main DataFrame\n",
    "data = data.join(genre_columns)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['title', 'genres', 'movie_id', 'zip'])\n",
    "\n",
    "# Fill missing year values with the median year\n",
    "data['year'].fillna(data['year'].median(), inplace=True)\n",
    "\n",
    "data['gender'] = data['gender'].apply(lambda x: x == 'M')\n",
    "data.drop(columns=['timestamp'], inplace=True)\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the resulting DataFrames to check the changes\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def generate_pairs(data, k=100_000):\n",
    "    grouped_data = data.groupby('user_id')\n",
    "    all_dfs = []\n",
    "    for user_idx, sub_df in grouped_data:\n",
    "        total_users = sub_df.shape[0]\n",
    "        all_combinations = list(itertools.combinations(range(total_users), 2))\n",
    "        min_samples = 1 if k//len(grouped_data) < 1 else k//len(grouped_data)\n",
    "        selected_combinations_idx = np.random.choice(range(len(all_combinations)), min(min_samples, len(all_combinations)), replace=False)\n",
    "        \n",
    "        selected_combinations = [all_combinations[i] for i in selected_combinations_idx]\n",
    "        for combination in selected_combinations:\n",
    "            # print(sub_df.iloc[combination[0:1]], sub_df.iloc[combination[1]])\n",
    "            all_dfs.append(pd.merge(sub_df.iloc[combination[0]:combination[0]+1], sub_df.iloc[combination[1]:combination[1]+1], on='user_id', suffixes=('_1', '_2')))\n",
    "\n",
    "    return pd.concat(all_dfs, axis=0)\n",
    "\n",
    "train_pairs = generate_pairs(train_data, k=100_000)\n",
    "test_pairs = generate_pairs(test_data, k=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'rating_1', 'gender_1', 'age_1', 'occupation_1', 'year_1',\n",
       "       'Action_1', 'Adventure_1', 'Animation_1', 'Children's_1', 'Comedy_1',\n",
       "       'Crime_1', 'Documentary_1', 'Drama_1', 'Fantasy_1', 'Film-Noir_1',\n",
       "       'Horror_1', 'Musical_1', 'Mystery_1', 'Romance_1', 'Sci-Fi_1',\n",
       "       'Thriller_1', 'War_1', 'Western_1', 'rating_2', 'gender_2', 'age_2',\n",
       "       'occupation_2', 'year_2', 'Action_2', 'Adventure_2', 'Animation_2',\n",
       "       'Children's_2', 'Comedy_2', 'Crime_2', 'Documentary_2', 'Drama_2',\n",
       "       'Fantasy_2', 'Film-Noir_2', 'Horror_2', 'Musical_2', 'Mystery_2',\n",
       "       'Romance_2', 'Sci-Fi_2', 'Thriller_2', 'War_2', 'Western_2', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs['label'] = train_pairs['rating_1'] > train_pairs['rating_2']\n",
    "test_pairs['label'] = test_pairs['rating_1'] > test_pairs['rating_2']\n",
    "test_pairs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PairwiseMovieDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "        self.scaler = StandardScaler()\n",
    "        features = pairs.drop(columns='label')\n",
    "        self.scaler.fit(features)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs.iloc[idx]\n",
    "        user_features = torch.tensor([\n",
    "            row['gender_1'], row['age_1'], row['occupation_1'],\n",
    "            row['rating_2'], row['gender_2'], row['age_2'], row['occupation_2']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        film1_features = torch.tensor([\n",
    "            row['year_1'], row['Action_1'], row['Adventure_1'], row['Animation_1'],\n",
    "            row['Children\\'s_1'], row['Comedy_1'], row['Crime_1'], row['Documentary_1'],\n",
    "            row['Drama_1'], row['Fantasy_1'], row['Film-Noir_1'], row['Horror_1'],\n",
    "            row['Musical_1'], row['Mystery_1'], row['Romance_1'], row['Sci-Fi_1'],\n",
    "            row['Thriller_1'], row['War_1'], row['Western_1']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        film2_features = torch.tensor([\n",
    "            row['year_2'], row['Action_2'], row['Adventure_2'], row['Animation_2'],\n",
    "            row['Children\\'s_2'], row['Comedy_2'], row['Crime_2'], row['Documentary_2'],\n",
    "            row['Drama_2'], row['Fantasy_2'], row['Film-Noir_2'], row['Horror_2'],\n",
    "            row['Musical_2'], row['Mystery_2'], row['Romance_2'], row['Sci-Fi_2'],\n",
    "            row['Thriller_2'], row['War_2'], row['Western_2']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
    "        \n",
    "        return user_features, film1_features, film2_features, label\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = PairwiseMovieDataset(train_pairs)\n",
    "test_dataset = PairwiseMovieDataset(test_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PairwiseRankingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PairwiseRankingModel, self).__init__()\n",
    "        # Define the architecture\n",
    "        self.fc1 = nn.Linear(7 + 19, 64)  # User features (7) + Film1 features (19)\n",
    "        self.fc2 = nn.Linear(64 + 19, 32)  # Add Film2 features (19)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output\n",
    "\n",
    "    def forward(self, user_features, film1_features, film2_features):\n",
    "        # Concatenate user features and film1 features\n",
    "        x = torch.cat([user_features, film1_features], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # Concatenate with film2 features\n",
    "        x = torch.cat([x, film2_features], dim=1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:24<00:00, 122.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6215, AUC: 0.6634, Accuracy: 0.6718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 158.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5608, AUC: 0.8650, Accuracy: 0.7090\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 141.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5071, AUC: 0.8050, Accuracy: 0.7516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 176.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6227, AUC: 0.8660, Accuracy: 0.6569\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 139.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4877, AUC: 0.8227, Accuracy: 0.7603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 168.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4712, AUC: 0.8662, Accuracy: 0.7542\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 141.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4804, AUC: 0.8290, Accuracy: 0.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 171.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5688, AUC: 0.8671, Accuracy: 0.7262\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 140.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4768, AUC: 0.8324, Accuracy: 0.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 167.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4572, AUC: 0.8666, Accuracy: 0.7881\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 142.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4721, AUC: 0.8362, Accuracy: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 161.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4481, AUC: 0.8666, Accuracy: 0.7727\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 141.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4685, AUC: 0.8400, Accuracy: 0.7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 167.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5059, AUC: 0.8666, Accuracy: 0.7491\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 139.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4667, AUC: 0.8415, Accuracy: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 167.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4399, AUC: 0.8656, Accuracy: 0.7851\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:22<00:00, 135.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4631, AUC: 0.8445, Accuracy: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 167.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4421, AUC: 0.8650, Accuracy: 0.7822\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3020/3020 [00:21<00:00, 140.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4624, AUC: 0.8448, Accuracy: 0.7729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:01<00:00, 163.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4392, AUC: 0.8662, Accuracy: 0.7885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate the model and move to CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PairwiseRankingModel().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_preds = []\n",
    "    for user_features, film1_features, film2_features, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Move data to the same device as the model\n",
    "        user_features = user_features.to(device)\n",
    "        film1_features = film1_features.to(device)\n",
    "        film2_features = film2_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(user_features, film1_features, film2_features).squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss and collect outputs for metrics\n",
    "        running_loss += loss.item() * user_features.size(0)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_outputs.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        all_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Training loss: {epoch_loss:.4f}, AUC: {auc:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for user_features, film1_features, film2_features, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            user_features = user_features.to(device)\n",
    "            film1_features = film1_features.to(device)\n",
    "            film2_features = film2_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(user_features, film1_features, film2_features).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * user_features.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_preds.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Validation loss: {epoch_loss:.4f}, AUC: {auc:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    evaluate(model, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
